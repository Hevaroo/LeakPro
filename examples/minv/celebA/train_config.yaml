run: # Configurations for a specific run
  random_seed: 1234 # Integer number of specifying random seed
  log_dir: target # String for indicating where to save all the information, including models and computed signals. We can reuse the models saved in the same log_dir.

train: # Configuration for training
  epochs: 30 # Integer number for indicating the epochs for training target model. For speedyresnet, it uses its own number of epochs.
  batch_size: 64 # Integer number for indicating batch size for training the target model. For speedyresnet, it uses its own batch size.
  optimizer: SGD # String which indicates the optimizer. We support Adam and SGD. For speedyresnet, it uses its own optimizer.
  learning_rate: 0.001 # Float number for indicating learning rate for training the target model. For speedyresnet, it uses its own learning_rate.
  momentum: 0.9
  weight_decay: 0.001 # Float number for indicating weight decay for training the target model. For speedyresnet, it uses its own weight_decay.

data: # Configuration for data
  dataset: celebA # String indicates the name of the dataset
  f_train: 1.0 # Float number from 0 to 1 indicating the fraction of the train dataset
  f_test: 0.0 # Float number from 0 to 1 indicating the size of the test set
  data_dir: ./data # String about where to save the data.

gan:
  n_dis: 2 # Integer number for indicating the number of discriminator updates per generator update
  iterations: 5000 # Integer number for indicating the iterations for training the GAN
  batch_size: 128 # Integer number for indicating batch size for training the GAN
  gen_lr: 0.0005 # Float number for indicating learning rate for training the generator
  dis_lr: 0.0002 # Float number for indicating learning rate for training the discriminator
  beta1: 0.0 # Float number for indicating beta1 for Adam optimizer
  beta2: 0.9 # Float number for indicating beta2 for Adam optimizer
  dim_z: 256 # Integer number for indicating the dimension of the latent space
  alpha: 0.2 # Float number for indicating the weight of the gradient penalty
  log_interval: 10 # Integer number for indicating the interval for logging the information