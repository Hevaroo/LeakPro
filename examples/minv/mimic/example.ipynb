{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cc30b5c",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f65d223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Path to the dataset zip file\n",
    "data_folder = \"./data\"\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from examples.minv.mimic.utils.preprocess_mimic_data import preprocess_data, extract_features_and_split\n",
    "\n",
    "input_path = os.path.join(data_folder, \"df.pkl\")\n",
    "lab_events_path = os.path.join(data_folder, \"lab_events_grouped.pkl\")\n",
    "\n",
    "# List of all possible continuous columns in MIMIC\n",
    "# Note: Not all of these columns are guaranteed to be present in the dataset after feature selection\n",
    "continuous_col_names = ['length_of_stay', 'num_procedures', 'num_medications', 'BMI',\n",
    "       'BMI (kg/m2)', 'Height', 'Height (Inches)', 'Weight', 'Weight (Lbs)',\n",
    "       'eGFR', 'systolic', 'diastolic']\n",
    "\n",
    "# Creates processed_data.pkl\n",
    "processed_path = preprocess_data(input_path, lab_events_path, continuous_col_names, mean_imputation=True)\n",
    "\n",
    "# Feature selection\n",
    "desired_num_unique_classes = 35\n",
    "# Creates private private_df.pkl and public_df.pkl\n",
    "extract_features_and_split(processed_path, desired_num_unique_classes, print_classification_reports=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bcf8af",
   "metadata": {},
   "source": [
    "# Target Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066c5e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import warnings\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import CategoryEmbeddingModelConfig, TabNetModelConfig, GANDALFConfig\n",
    "from pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig\n",
    "import pandas as pd\n",
    "\n",
    "# Suppres warnings, pytorch_tabular is very verbose\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Redefine variables in case upper cell is not run\n",
    "# Path to the dataset zip file\n",
    "data_folder = \"./data\"\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# List of all possible continuous columns in MIMIC\n",
    "continuous_col_names = ['length_of_stay', 'num_procedures', 'num_medications', 'BMI',\n",
    "       'BMI (kg/m2)', 'Height', 'Height (Inches)', 'Weight', 'Weight (Lbs)',\n",
    "       'eGFR', 'systolic', 'diastolic']\n",
    "\n",
    "\n",
    "# Load the config.yaml file\n",
    "with open('train_config.yaml', 'r') as file:\n",
    "    train_config = yaml.safe_load(file)\n",
    "\n",
    "with open('audit.yaml', 'r') as file:\n",
    "    audit_config = yaml.safe_load(file)\n",
    "\n",
    "num_classes = audit_config[\"audit\"][\"attack_list\"][\"plgmi\"][\"num_classes\"]\n",
    "\n",
    "# Generate the dataset and dataloaders\n",
    "path = os.path.join(os.getcwd(), train_config[\"data\"][\"data_dir\"])\n",
    "data_dir =  train_config[\"data\"][\"data_dir\"] + \"/private_df.pkl\"\n",
    "\n",
    "df = pd.read_pickle(data_dir)\n",
    "\n",
    "# Reset index to have a clean, sequential integer index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Remove the columns from continuous_col_names that are not in the dataframe\n",
    "continuous_col_names = [col for col in continuous_col_names if col in df.columns]\n",
    "# Categorical column names are all columns that are not continuous\n",
    "categorical_col_names = [col for col in df.columns if col not in continuous_col_names]\n",
    "# Remove the target column\n",
    "categorical_col_names.remove(\"identity\")\n",
    "\n",
    "# Ensure df_train contains at least one sample for every class:\n",
    "df_train_min = df.groupby(\"identity\").head(1)  # uses the new, clean index\n",
    "remaining_df = df.drop(df_train_min.index)      # Now, the indices align perfectly\n",
    "\n",
    "# Determine the fraction for the remaining samples:\n",
    "desired_frac = train_config[\"data\"][\"f_train\"]\n",
    "frac_remaining = desired_frac - (len(df_train_min) / len(df))\n",
    "df_train_remaining = remaining_df.sample(frac=frac_remaining, random_state=123)\n",
    "\n",
    "# Merge the guaranteed and random samples.\n",
    "# Note: we keep the original indices here (do not use ignore_index) so that we can compute df_val correctly.\n",
    "train_indices = df_train_min.index.union(df_train_remaining.index)\n",
    "df_train = df.loc[train_indices]\n",
    "\n",
    "# Create df_val by taking the rest of the samples\n",
    "df_test = df.drop(train_indices)\n",
    "df_test = df_test[df_test[\"identity\"].isin(df_train[\"identity\"])]\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "# Prints\n",
    "print(\"Number of unique classes in df_train: \", df_train[\"identity\"].nunique())\n",
    "print(\"Shape of df_train: \", df_train.shape)\n",
    "print(\"Shape of df_test: \", df_test.shape)\n",
    "\n",
    "\n",
    "data_config = DataConfig(\n",
    "    target=['identity'],\n",
    "    continuous_cols=continuous_col_names,\n",
    "    categorical_cols=categorical_col_names,\n",
    "    normalize_continuous_features=False,\n",
    ")\n",
    "\n",
    "trainer_config = TrainerConfig(\n",
    "    auto_lr_find=False,\n",
    "    batch_size=256,\n",
    "    max_epochs=100,\n",
    "    early_stopping='train_loss_0',\n",
    ")\n",
    "\n",
    "optimizer_config = OptimizerConfig()\n",
    "\n",
    "model_config = CategoryEmbeddingModelConfig(\n",
    "    task=\"classification\",\n",
    "    layers=\"2048-1024-512-256\",\n",
    "    activation=\"ReLU\",\n",
    "    learning_rate=1e-3,\n",
    ")\n",
    "\n",
    "# model_config = GANDALFConfig(\n",
    "# task=\"classification\",\n",
    "# gflu_stages=16,\n",
    "# gflu_dropout=0.1,\n",
    "# embedding_dropout=0.1,\n",
    "# learning_rate=1e-3,\n",
    "# )\n",
    "\n",
    "tabular_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config\n",
    ")\n",
    "\n",
    "tabular_model.fit(train=df_train) # Defaults 80% train, 20% val split\n",
    "results = tabular_model.evaluate(df_test)\n",
    "pred_df = tabular_model.predict(df_test.drop(columns=[\"identity\"]))\n",
    "\n",
    "# Save the model\n",
    "tabular_model.save_model(\"./target/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e96067",
   "metadata": {},
   "source": [
    "# Plot training/val loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebee0cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_dir = tabular_model.trainer.logger.log_dir\n",
    "\n",
    "# Path to the TensorBoard log directory\n",
    "event_file = f\"{log_dir}/events.out.tfevents.*\"\n",
    "\n",
    "# Load the event file\n",
    "event_acc = EventAccumulator(log_dir)\n",
    "event_acc.Reload()\n",
    "\n",
    "# List all available scalar metrics\n",
    "print(\"Available metrics:\", event_acc.Tags()[\"scalars\"])\n",
    "# Extract training and validation loss\n",
    "train_acc = [scalar.value for scalar in event_acc.Scalars(\"train_accuracy\")]\n",
    "val_acc = [scalar.value for scalar in event_acc.Scalars(\"valid_accuracy\")]\n",
    "test_acc = [scalar.value for scalar in event_acc.Scalars(\"test_accuracy\")]\n",
    "\n",
    "print(\"Test accuracy: \", test_acc[0])\n",
    "\n",
    "# Plot the metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_acc, label=\"Training Loss\")\n",
    "plt.plot(val_acc, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34f5446",
   "metadata": {},
   "source": [
    "# LeakPro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e080de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Suppres warnings, pytorch_tabular is very verbose\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from leakpro import LeakPro\n",
    "from examples.minv.mimic.mimic_plgmi_handler import Mimic_InputHandler\n",
    "config_path = \"audit.yaml\"\n",
    "\n",
    "# Initialize the LeakPro object\n",
    "leakpro = LeakPro(Mimic_InputHandler, config_path)\n",
    "\n",
    "# Run the audit\n",
    "results = leakpro.run_audit(return_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c535b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If gower distance test was run, execute this cell to see best rows\n",
    "print(results[0].best_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869ecdb9",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ac5475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'\n",
    "\n",
    "for result in results[0].numerical_plots:\n",
    "    # Get the plot\n",
    "    plot = results[0].numerical_plots[result]\n",
    "    # Show the plot\n",
    "    plot.show()\n",
    "\n",
    "\n",
    "for result in results[0].categorical_plots:\n",
    "    # Get the plot\n",
    "    plot = results[0].categorical_plots[result]\n",
    "    # Show the plot\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c856666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].results['quality_report'].get_details(property_name='Column Pair Trends')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b621bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'\n",
    "\n",
    "fig = results[0].results['quality_report'].get_visualization(property_name=\"Column Shapes\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
