{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRUD with PDSGD\n",
    "In this example, we assess the risk of data leakage of the GRUD target model trained with DPSGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Target Model\n",
    "\n",
    "### Installation of Packages in Conda\n",
    "\n",
    "To install the required packages in your conda environment, you can use the following commands:\n",
    "\n",
    "```bash\n",
    "conda install h5py\n",
    "conda install pytables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.data_processing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m project_root \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../..\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      7\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(project_root)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_processing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_mimic_dataloaders, get_mimic_dataset\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils.data_processing'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from torch import zeros\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from utils.data_processing import get_mimic_dataloaders, get_mimic_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset and dataloaders\n",
    "path = os.path.join(os.getcwd(), \"data/\")\n",
    "\n",
    "train_frac = 0.4\n",
    "valid_frac = 0.0\n",
    "test_frac = 0.0\n",
    "early_stop_frac = 0.4\n",
    "batch_size = 59\n",
    "use_LR = False # True if you want to use the LR model, False if you want to use the GRUD model\n",
    "\n",
    "dataset, train_indices, validation_indices, test_indices, early_stop_indices= get_mimic_dataset(path,\n",
    "                                                                            train_frac ,\n",
    "                                                                            valid_frac,\n",
    "                                                                            test_frac,\n",
    "                                                                            early_stop_frac,\n",
    "                                                                            use_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, validation_loader, test_loader, early_stop_loader = get_mimic_dataloaders(dataset,\n",
    "                                                            train_indices,\n",
    "                                                            validation_indices,\n",
    "                                                            test_indices,\n",
    "                                                            early_stop_indices,\n",
    "                                                            batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_hyperparams ={\n",
    "    \"cell_size\": 58,\n",
    "    \"hidden_size\": 78,\n",
    "    \"learning_rate\": 0.0004738759319792616,\n",
    "    \"num_epochs\":37,\n",
    "    \"patience_early_stopping\": 20,\n",
    "    \"patience_lr_scheduler\": 5,\n",
    "    \"batch_size\": 59,\n",
    "    \"seed\": 4410,\n",
    "    \"min_delta\": 0.00001,\n",
    "    }\n",
    "n_features = int(dataset.x.shape[1]/3)\n",
    "X_mean = zeros(1,dataset.x.shape[2],n_features)\n",
    "\n",
    "model_params = {k: optimized_hyperparams[k] for k in [\"cell_size\", \"hidden_size\", \"batch_size\"]}\n",
    "\n",
    "# Add other required parameters to model_params\n",
    "model_params.update({\n",
    "    \"input_size\": n_features,\n",
    "    \"X_mean\": X_mean,\n",
    "    \"output_last\": False\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_grud import *\n",
    "\n",
    "# Initialize the model with filtered parameters\n",
    "model = GRUD(**model_params)\n",
    "\n",
    "# Train the model with Train_Model function\n",
    "train_losses, test_losses , train_acc, test_acc = gru_trained_model_and_metadata(model,\n",
    "                                                                                train_loader,\n",
    "                                                                                early_stop_loader,\n",
    "                                                                                epochs = optimized_hyperparams[\"num_epochs\"],\n",
    "                                                                                patience_early_stopping = optimized_hyperparams[\"patience_early_stopping\"],\n",
    "                                                                                patience_lr= optimized_hyperparams[\"patience_lr_scheduler\"],\n",
    "                                                                                min_delta = optimized_hyperparams[\"min_delta\"],\n",
    "                                                                                learning_rate = optimized_hyperparams[\"learning_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert losses to numpy-compatible lists directly\n",
    "train_losses_cpu = [float(loss) for loss in train_losses]\n",
    "test_losses_cpu = [float(loss) for loss in test_losses]\n",
    "\n",
    "# Plot training and test accuracy\n",
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_acc, label=\"Train Accuracy\")\n",
    "plt.plot(test_acc, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and test loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Attacking the GRUD-dpsgd model\n",
    "Modify ```audit.yaml ``` file to attack GRUD model: \n",
    "  \n",
    "  ```\n",
    "  module_path: \"utils/model_grud.py\" \n",
    "  model_class: \"GRUD\"\n",
    "  target_folder: \"./target_GRUD\"\n",
    "  data_path: \"./data/unflattened/dataset.pkl\"\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mimic_gru_handler import MimicInputHandlerGRU\n",
    "\n",
    "from leakpro import LeakPro\n",
    "\n",
    "# Read the config file\n",
    "config_path = \"audit.yaml\"\n",
    "\n",
    "# Prepare leakpro object\n",
    "leakpro = LeakPro(MimicInputHandlerGRU, config_path)\n",
    "\n",
    "# Run the audit\n",
    "mia_results = leakpro.run_audit(return_results=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leakpro_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
